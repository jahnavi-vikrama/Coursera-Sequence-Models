# Sequence-Models

## Certificate : https://coursera.org/share/7108222f531c8057d1a22a6bfca07aaf

## Week 1
Defined notation for building sequence models.\
Described the architecture of a basic RNN.\
Identified the main components of an LSTM.\
Implemented backpropagation through time for a basic RNN and an LSTM.\
Provided examples of several types of RNNs.\
Built a character-level text generation model using an RNN.\
Stored text data for processing using an RNN.\
Sampled novel sequences in an RNN.\
Explained the vanishing/exploding gradient problem in RNNs.\
Applied gradient clipping as a solution for exploding gradients.\
Described the architecture of a GRU.\
Used a bidirectional RNN to take information from two points of a sequence.\
Stacked multiple RNNs to create a deep RNN.\
Utilized the flexible Functional API to create complex models.\
Generated jazz music using deep learning techniques.\
Applied an LSTM to a music generation task.\


## Week 2
Explained how word embeddings capture relationships between words.\
Loaded pre-trained word vectors.\
Measured similarity between word vectors using cosine similarity.\
Used word embeddings to solve word analogy problems.\
Worked on reducing bias in word embeddings.\
Created an embedding layer in Keras with pre-trained word vectors.\
Described how negative sampling learns word vectors more efficiently.\
Explained the advantages and disadvantages of the GloVe algorithm.\
Built a sentiment classifier using word embeddings.\
Developed and trained a more sophisticated classifier using an LSTM.\


## Week 3
Described a basic sequence-to-sequence model.\
Compared and contrasted different algorithms for language translation.\
Optimized beam search and analyzed it for errors.\
Used beam search to identify likely translations.\
Applied BLEU score to evaluate machine-translated text.\
Implemented an attention model.\
Trained a trigger word detection model and made predictions.\
Synthesized and processed audio recordings to create train/dev datasets.\
Structured a speech recognition project.\


## Week 4
Created positional encodings to capture sequential relationships in data.\
Calculated scaled dot-product self-attention with word embeddings.\
Implemented masked multi-head attention.\
Built and trained a Transformer model.\
Fine-tuned a pre-trained transformer model for Named Entity Recognition.\
Fine-tuned a pre-trained transformer model for Question Answering.\
Implemented a QA model in TensorFlow and PyTorch.\
Fine-tuned a pre-trained transformer model to a custom dataset.\
Performed extractive Question Answering.\
